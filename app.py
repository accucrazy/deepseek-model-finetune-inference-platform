import streamlit as st
import json
import os
import torch
import plotly.graph_objects as go
from datetime import datetime
import glob
from inference import generate_response, load_model

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="The Pocket Company",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        color: #FF1493;
        text-align: center;
    }
    
    .sub-header {
        font-size: 1.8rem;
        color: #FF1493;
        margin-bottom: 0.5rem;
        text-align: center;
    }
    
    .platform-name {
        font-size: 1.2rem;
        color: #FF1493;
        margin-bottom: 2rem;
        text-align: center;
    }
    
    .company-signature {
        font-size: 0.9rem;
        color: #FF1493;
        text-align: right;
        margin-top: 1rem;
        padding-right: 1rem;
        position: fixed;
        bottom: 20px;
        right: 20px;
        background-color: rgba(255, 255, 255, 0.9);
        padding: 5px 10px;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    </style>
    """, unsafe_allow_html=True)

# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ¢ The Pocket Company by Accucrazy è‚–æº–</h1>', unsafe_allow_html=True)
st.markdown('<h2 class="sub-header">ğŸ¤– Deepseekæ¨¡å‹èª¿æ•™èˆ‡æ¨è«–å¹³å°</h2>', unsafe_allow_html=True)
st.markdown('<h3 class="platform-name">ğŸš€ Deepseek Model Fine-tuning and Inference Platform</h3>', unsafe_allow_html=True)

# æ·»åŠ å›ºå®šçš„å“ç‰Œæ ‡è¯†
st.markdown('<p class="company-signature">ğŸ¢ The Pocket Company by Accucrazy è‚–æº–</p>', unsafe_allow_html=True)

# é¡¯ç¤º CUDA ç‹€æ…‹
cuda_status = st.empty()
with cuda_status.container():
    st.subheader("ğŸ’» ç³»çµ±ç‹€æ…‹")
    col1, col2 = st.columns(2)
    
    with col1:
        if torch.cuda.is_available():
            st.success("âœ¨ CUDA å¯ç”¨")
            st.info(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        else:
            st.error("âŒ CUDA ä¸å¯ç”¨")
            st.warning("âš ï¸ è«‹ç¢ºä¿ï¼š\n1. å·²å®‰è£ NVIDIA é©…å‹•\n2. CUDA å·¥å…·åŒ…å·²æ­£ç¢ºå®‰è£\n3. PyTorch å·²å®‰è£ CUDA ç‰ˆæœ¬")
    
    with col2:
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            memory_reserved = torch.cuda.memory_reserved() / 1024**3
            st.info(f"ğŸ“Š GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_allocated:.2f} GB")
            st.info(f"ğŸ’¾ GPU è¨˜æ†¶é«”ä¿ç•™: {memory_reserved:.2f} GB")

# åˆ›å»ºä¸¤ä¸ªæ ‡ç­¾é¡µ
tab1, tab2 = st.tabs(["âš¡ æ¨¡å‹è¨“ç·´", "ğŸ”® æ¨¡å‹æ¨ç†"])

with tab1:
    st.header("âš¡ æ¨¡å‹è¨“ç·´")
    
    # å´é‚Šæ¬„é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ è¨“ç·´é…ç½®")
        
        # æ¨¡å‹é€‰æ‹©
        model_name = st.selectbox(
            "ğŸ¤– é¸æ“‡åŸºç¤æ¨¡å‹",
            ["deepseek-ai/deepseek-coder-1.3b-base",
             "deepseek-ai/deepseek-coder-6.7b-base",
             "deepseek-ai/deepseek-coder-33b-base"],
            help="é¸æ“‡è¦ fine-tune çš„åŸºç¤æ¨¡å‹"
        )
        
        # è®­ç»ƒå‚æ•°è®¾ç½®
        st.subheader("ğŸ“Š è¨“ç·´åƒæ•¸")
        col1, col2 = st.columns(2)
        with col1:
            num_epochs = st.number_input("ğŸ”„ è¨“ç·´è¼ªæ•¸", min_value=1, value=3)
            batch_size = st.number_input("ğŸ“¦ Batch Size", min_value=1, value=4)
        with col2:
            learning_rate = st.number_input("ğŸ“ˆ å­¸ç¿’ç‡", min_value=0.0, value=2e-5, format="%.5f")
            max_length = st.number_input("ğŸ“ æœ€å¤§åºåˆ—é•·åº¦", min_value=32, value=256)
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    st.subheader("ğŸ“¥ æº–å‚™è¨“ç·´æ•¸æ“š")
    
    # æ•°æ®é›†é€‰æ‹©æ–¹å¼
    dataset_source = st.radio(
        "ğŸ’¿ é¸æ“‡æ•¸æ“šé›†ä¾†æº",
        ["ğŸ“ å¾è³‡æ–™å¤¾é¸æ“‡", "ğŸ“¤ ä¸Šå‚³æ–°æª”æ¡ˆ"]
    )
    
    dataset = None
    dataset_path = None
    
    if dataset_source == "ğŸ“ å¾è³‡æ–™å¤¾é¸æ“‡":
        # åªé¡¯ç¤º qa_dataset.jsonl
        json_files = ["temp_datasets/qa_dataset.jsonl"]
        
        if json_files:
            selected_file = st.selectbox(
                "é¸æ“‡æ•¸æ“šé›†æ–‡ä»¶",
                json_files,
                format_func=lambda x: f"ğŸ“„ {os.path.basename(x)} ({x})"
            )
            
            if selected_file:
                try:
                    with open(selected_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        train_data = [json.loads(line.strip()) for line in lines if line.strip()]
                        dataset_path = selected_file
                        dataset = {"train": train_data}
                        st.success(f"âœ… æˆåŠŸè¼‰å…¥æ•¸æ“šé›†ï¼š{selected_file}")
                except Exception as e:
                    st.error(f"âŒ è¼‰å…¥æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        else:
            st.warning("âš ï¸ æ‰¾ä¸åˆ° qa_dataset.jsonl æ–‡ä»¶")
    
    else:
        uploaded_file = st.file_uploader("ä¸Šå‚³è¨“ç·´æ•¸æ“šé›†", type=['jsonl'])
        if uploaded_file is not None:
            try:
                temp_dir = "temp_datasets"
                os.makedirs(temp_dir, exist_ok=True)
                temp_path = os.path.join(temp_dir, uploaded_file.name)
                
                with open(temp_path, 'wb') as f:
                    f.write(uploaded_file.getvalue())
                
                dataset_path = temp_path
                with open(temp_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    train_data = [json.loads(line.strip()) for line in lines if line.strip()]
                    dataset = {"train": train_data}
                
                st.success("âœ… æ•¸æ“šé›†ä¸Šå‚³ä¸¦è™•ç†æˆåŠŸï¼")
            except Exception as e:
                st.error(f"âŒ è¼‰å…¥æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    if dataset is not None:
        st.subheader("ğŸ“Š æ•¸æ“šé›†é è¦½")
        st.info(f"ğŸ“ˆ è¨“ç·´æ¨£æœ¬æ•¸é‡: {len(dataset.get('train', []))}")
        
        with st.expander("ğŸ‘€ æŸ¥çœ‹æ•¸æ“šæ¨£æœ¬"):
            st.json(dataset['train'][:2])
    
    # è®­ç»ƒéƒ¨åˆ†
    st.subheader("ğŸš€ é–‹å§‹è¨“ç·´")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ” æª¢æŸ¥ CUDA ç’°å¢ƒ", help="é»æ“Šä»¥ç²å–è©³ç´°çš„ CUDA ç’°å¢ƒä¿¡æ¯"):
            with st.expander("ğŸ’» CUDA ç’°å¢ƒè¨ºæ–·", expanded=True):
                st.write("=== CUDA ç’°å¢ƒè¨ºæ–· ===")
                st.write(f"ğŸ”§ PyTorch ç‰ˆæœ¬: {torch.__version__}")
                st.write(f"âœ¨ CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
                if torch.cuda.is_available():
                    st.write(f"ğŸ’ª GPU æ•¸é‡: {torch.cuda.device_count()}")
                    for i in range(torch.cuda.device_count()):
                        st.write(f"ğŸ® GPU {i}: {torch.cuda.get_device_name(i)}")
                    st.write(f"ğŸ“Œ ç•¶å‰ GPU: {torch.cuda.current_device()}")
                    st.write(f"ğŸ“Š GPU è¨˜æ†¶é«”ä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
                    st.write(f"ğŸ’¾ GPU è¨˜æ†¶é«”å¿«å–: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
    
    with col2:
        if st.button("ğŸš€ é–‹å§‹ Fine-tuning", disabled=dataset is None):
            if not torch.cuda.is_available():
                st.warning("âš ï¸ è­¦å‘Šï¼šæœªæª¢æ¸¬åˆ° GPUï¼Œè¨“ç·´å¯èƒ½æœƒå¾ˆæ…¢")
            
            output_dir = f"fine_tuned_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            os.makedirs(output_dir, exist_ok=True)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                from train import train
                status_text.text("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è¨“ç·´...")
                trainer = train(
                    model_name=model_name,
                    dataset_path=dataset_path,
                    output_dir=output_dir,
                    num_train_epochs=num_epochs,
                    per_device_train_batch_size=batch_size,
                    learning_rate=learning_rate,
                    max_length=max_length
                )
                
                st.success(f"âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")
                
                if trainer.state.log_history:
                    train_loss = [log.get('loss', 0) for log in trainer.state.log_history if 'loss' in log]
                    eval_loss = [log.get('eval_loss', 0) for log in trainer.state.log_history if 'eval_loss' in log]
                    
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(y=train_loss, name="è¨“ç·´æå¤±"))
                    if eval_loss:
                        fig.add_trace(go.Scatter(y=eval_loss, name="é©—è­‰æå¤±"))
                    fig.update_layout(title="ğŸ“ˆ è¨“ç·´éç¨‹", xaxis_title="æ­¥é©Ÿ", yaxis_title="æå¤±")
                    st.plotly_chart(fig)
                    
            except Exception as e:
                st.error(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                st.exception(e)

with tab2:
    st.header("ğŸ”® æ¨¡å‹æ¨ç†")
    
    # æ¨¡å‹é€‰æ‹©
    model_folders = []
    for folder in os.listdir():
        if folder.startswith('fine_tuned_model_'):
            for checkpoint in os.listdir(folder):
                if checkpoint.startswith('checkpoint-'):
                    checkpoint_path = os.path.join(folder, checkpoint)
                    if os.path.exists(os.path.join(checkpoint_path, 'config.json')):
                        model_folders.append(checkpoint_path)
    
    selected_model = st.selectbox("ğŸ¤– é€‰æ‹©æ¨¡å‹", model_folders)
    
    # ç”Ÿæˆå‚æ•°è®¾ç½®
    st.subheader("âš™ï¸ ç”Ÿæˆå‚æ•°è®¾ç½®")
    col1, col2 = st.columns(2)
    with col1:
        temperature = st.slider("ğŸŒ¡ï¸ Temperature", 0.0, 2.0, 0.6, 0.1, 
            help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚å€¼è¶Šé«˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šéšæœºï¼›å€¼è¶Šä½ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šç¡®å®šã€‚")
        max_new_tokens = st.slider("ğŸ“ Maximum New Tokens", 1, 2000, 100, 
            help="ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€‚")
    with col2:
        top_p = st.slider("ğŸ“Š Top P", 0.0, 1.0, 0.9, 0.1,
            help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚å€¼è¶Šé«˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šå¤šæ ·ï¼›å€¼è¶Šä½ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šä¿å®ˆã€‚")
        repetition_penalty = st.slider("ğŸ”„ Repetition Penalty", 1.0, 2.0, 1.9, 0.1,
            help="æ§åˆ¶é‡å¤æƒ©ç½šç¨‹åº¦ã€‚å€¼è¶Šé«˜ï¼Œè¶Šä¸å®¹æ˜“é‡å¤ï¼›å€¼è¶Šä½ï¼Œè¶Šå®¹æ˜“é‡å¤ã€‚")
    
    # é—®é¢˜è¾“å…¥
    question = st.text_area("ğŸ’­ è¾“å…¥é—®é¢˜", height=100)
    
    # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥å­˜æ”¾å›ç­”å’Œç­¾å
    response_container = st.container()
    
    if st.button("ğŸ¤– ç”Ÿæˆå›ç­”"):
        if not selected_model:
            st.error("âš ï¸ è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
        elif not question:
            st.error("âš ï¸ è¯·è¾“å…¥é—®é¢˜")
        else:
            try:
                with st.spinner("âŒ› æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                    model, tokenizer = load_model(selected_model)
                    
                with st.spinner("ğŸ¤” æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                    response = generate_response(
                        model=model,
                        tokenizer=tokenizer,
                        question=question,
                        temperature=temperature,
                        top_p=top_p,
                        max_new_tokens=max_new_tokens,
                        repetition_penalty=repetition_penalty
                    )
                    with response_container:
                        st.markdown("### âœ¨ å›ç­”")
                        st.write(response)
                        st.markdown('<p class="company-signature">ğŸ¢ The Pocket Company by Accucrazy è‚–æº–</p>', unsafe_allow_html=True)
            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.exception(e)

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
with st.expander("ğŸ“– ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    ### ğŸ”„ æ¨¡å‹è¨“ç·´
    1. **é¸æ“‡æ•¸æ“šé›†**ï¼š
        - ğŸ“ å¾è³‡æ–™å¤¾é¸æ“‡ï¼šé¸æ“‡å·²å­˜åœ¨çš„ JSON/JSONL æ–‡ä»¶
        - ğŸ“¤ ä¸Šå‚³æ–°æª”æ¡ˆï¼šä¸Šå‚³æ–°çš„æ•¸æ“šé›†æ–‡ä»¶
    
    2. **é…ç½®è¨“ç·´åƒæ•¸**ï¼š
        - âš™ï¸ åœ¨å´é‚Šæ¬„ä¸­è¨­ç½®è¨“ç·´åƒæ•¸
        - ğŸ® æ ¹æ“šæ‚¨çš„ GPU è¨˜æ†¶é«”å¤§å°èª¿æ•´ batch size
    
    3. **é–‹å§‹è¨“ç·´**ï¼š
        - ğŸš€ é»æ“Š"é–‹å§‹ Fine-tuning"æŒ‰éˆ•
        - â³ ç­‰å¾…è¨“ç·´å®Œæˆ
    
    ### ğŸ¤– æ¨¡å‹æ¨ç†
    1. **é¸æ“‡æ¨¡å‹**ï¼š
        - ğŸ“ å¾å·²è¨“ç·´çš„æ¨¡å‹ä¸­é¸æ“‡
    
    2. **è¼¸å…¥å•é¡Œ**ï¼š
        - âœï¸ åœ¨æ–‡æœ¬æ¡†ä¸­è¼¸å…¥æ‚¨çš„å•é¡Œ
    
    3. **ç”Ÿæˆå›ç­”**ï¼š
        - ğŸ¤– é»æ“Š"ç”Ÿæˆå›ç­”"æŒ‰éˆ•
        - ğŸ“ æŸ¥çœ‹æ¨¡å‹çš„å›ç­”
    
    ### âš ï¸ æ³¨æ„äº‹é …
    - ç¢ºä¿æ‚¨æœ‰è¶³å¤ çš„ GPU è¨˜æ†¶é«”
    - å¦‚æœé‡åˆ°è¨˜æ†¶é«”ä¸è¶³ï¼Œå¯ä»¥ï¼š
        - ğŸ“‰ æ¸›å°‘ batch size
        - ğŸ“‰ æ¸›å°‘æœ€å¤§åºåˆ—é•·åº¦
        - ğŸ“‰ é¸æ“‡è¼ƒå°çš„æ¨¡å‹ç‰ˆæœ¬
    """) 